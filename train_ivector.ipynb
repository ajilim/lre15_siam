{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,sys\n",
    "sys.path.insert(0, './scripts')\n",
    "dataDir ='./data'\n",
    "\n",
    "\n",
    "import py_compile\n",
    "py_compile.compile('scripts/ivector_dataset.py')\n",
    "py_compile.compile('scripts/siamese_model.py')\n",
    "py_compile.compile('scripts/ivector_tools.py')\n",
    "\n",
    "import ivector_dataset\n",
    "import siamese_model\n",
    "import ivector_tools as it\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ivectors(filename):\n",
    "    \"\"\"Loads ivectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        Path to ivector files (e.g. dev_ivectors.csv)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids : list\n",
    "        List of ivectorids\n",
    "    durations : array, shaped('n_ivectors')\n",
    "        Array of durations for each ivectorid\n",
    "    languages : array, shaped('n_ivectors')\n",
    "        Array of langs for each ivectorid (only applies to train)\n",
    "    ivectors : array, shaped('n_ivectors', 600)\n",
    "        Array of ivectors for each ivectorid\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    durations = []\n",
    "    languages = []\n",
    "    ivectors = []\n",
    "    with open(filename, 'rb') as infile:\n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        reader.next()\n",
    "\n",
    "        for row in csv.reader(infile, delimiter='\\t'):\n",
    "            ids.append(row[0])\n",
    "            durations.append(float(row[1]))\n",
    "            languages.append(row[2])\n",
    "            ivectors.append(np.asarray(row[3:], dtype=np.float32))\n",
    "\n",
    "#             sys.stdout.write(\"\\r     %s  \" % row[0])\n",
    "#             sys.stdout.flush()\n",
    "\n",
    "    print \"\\n   I-    Adding Transformed ivectors \"\n",
    "\n",
    "    return ids, np.array(durations, dtype=np.float32), np.array(languages), np.vstack(ivectors)\n",
    "\n",
    "\n",
    "# load ivector ids, durations , languages, and ivectors (as row vectors)\n",
    "print \"\\n 1. development data\"\n",
    "dev_ids, dev_durations, dev_languages, dev_ivec = \\\n",
    "    load_ivectors('./data/ivec15_lre_dev_ivectors.tsv')\n",
    "print \"\\n 2. training data\"\n",
    "train_ids, train_durations, train_languages, train_ivec = \\\n",
    "    load_ivectors('./data/ivec15_lre_train_ivectors.tsv')\n",
    "print \"\\n 3. test data\"\n",
    "test_ids, test_durations, test_languages, test_ivec = \\\n",
    "    load_ivectors('./data/ivec15_lre_test_ivectors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[languages,train_languages_num] = np.unique(train_languages,return_inverse=True)\n",
    "\n",
    "V = it.lda2(train_ivec,train_languages_num)\n",
    "V = V[:,0:49]\n",
    "dev_ivec = np.matmul(dev_ivec,V)\n",
    "train_ivec = np.matmul(train_ivec,V)\n",
    "test_ivec = np.matmul(test_ivec,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and whitening transformation over dev set only\n",
    "m = np.mean(dev_ivec, axis=0)\n",
    "S = np.cov(dev_ivec, rowvar=0)\n",
    "D, V = np.linalg.eig(S)\n",
    "W = (1 / np.sqrt(D) * V).transpose().astype('float32')\n",
    "\n",
    "# # center and whiten all i-vectors\n",
    "dev_ivec = np.dot(dev_ivec - m, W.transpose())\n",
    "train_ivec = np.dot(train_ivec - m, W.transpose())\n",
    "test_ivec = np.dot(test_ivec - m, W.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_ivec = np.zeros((len(np.unique(train_languages)), train_ivec.shape[1]))\n",
    "avg_train_languages = []\n",
    "for i, language in enumerate(np.unique(train_languages)):\n",
    "    avg_train_ivec[i] = np.mean(train_ivec[train_languages == language], axis=0)\n",
    "    avg_train_languages.append(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# project all i-vectors into unit sphere\n",
    "dev_ivec /= np.sqrt(np.sum(dev_ivec ** 2, axis=1))[:, np.newaxis]\n",
    "train_ivec /= np.sqrt(np.sum(train_ivec ** 2, axis=1))[:, np.newaxis]\n",
    "test_ivec /= np.sqrt(np.sum(test_ivec ** 2, axis=1))[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_languages = np.loadtxt('data/ivec15_lre_trial_key.v1.tsv',usecols=[1],dtype='string')\n",
    "test_set = np.loadtxt('data/ivec15_lre_trial_key.v1.tsv',usecols=[2],dtype='string')\n",
    "test_languages = test_languages[1:]\n",
    "test_set = test_set[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['out_of_set' 'thai' 'tagalog' ..., 'out_of_set' 'shona' 'zulu']\n"
     ]
    }
   ],
   "source": [
    "test_languages_num=np.zeros(len(test_ids),dtype=int)\n",
    "print test_languages\n",
    "\n",
    "for i,lang in enumerate(languages):\n",
    "    test_languages_num[ test_languages==languages[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518\n",
      "0.789\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "dev_scores = avg_train_ivec.dot(train_ivec.transpose() )\n",
    "hypo_lang = np.argmax(dev_scores,axis = 0)\n",
    "temp = ((train_languages_num) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float( np.size(train_languages_num) )\n",
    "print acc\n",
    "\n",
    "tst_scores = avg_train_ivec.dot(test_ivec.transpose() )\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "hypo_lang = hypo_lang[(test_languages=='out_of_set')==False]\n",
    "test_languages_num = test_languages_num[(test_languages=='out_of_set')==False]\n",
    "temp = ((test_languages_num) - hypo_lang)\n",
    "acc_tst =1- np.size(np.nonzero(temp)) / float(np.size(test_languages_num))\n",
    "\n",
    "print acc_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[  5.  74.  95. ...,   1.   1.   1.]\n",
      " [  1.   0.   0. ...,   1.   1.   0.]\n",
      " [  3.   0.   0. ...,   1.   0.   3.]\n",
      " ..., \n",
      " [  0.   0.   0. ...,   0.   1.   0.]\n",
      " [  2.   0.   0. ...,   1.   0.   0.]\n",
      " [  1.   3.   1. ...,  52.  63.  92.]]\n",
      "Precision\n",
      "[ 0.00883392  0.          0.          0.          0.03614458  0.          0.\n",
      "  0.          0.          0.          0.01052632  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.74757282  0.          0.\n",
      "  0.          0.          0.86538462  0.          0.          0.          0.\n",
      "  0.          0.33566434  0.          0.          0.          0.          0.\n",
      "  0.4787234   0.          0.          0.          0.          0.40340909\n",
      "  0.          0.          0.29496403  0.01041667  0.00990099  0.          0.\n",
      "  0.          0.17005545]\n",
      "Recall\n",
      "[ 0.05  0.    0.    0.    0.03  0.    0.    0.    0.    0.    0.01  0.    0.\n",
      "  0.    0.    0.    0.    0.    0.77  0.    0.    0.    0.    0.9   0.    0.\n",
      "  0.    0.    0.    0.48  0.    0.    0.    0.    0.    0.9   0.    0.    0.\n",
      "  0.    0.71  0.    0.    0.82  0.02  0.01  0.    0.    0.    0.92]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.789\n",
      "Precision : 0.067\n",
      "Recall    : 0.112\n"
     ]
    }
   ],
   "source": [
    "confusionmat = np.zeros((50,50))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ test_languages_num == i]\n",
    "    hist_bylang = np.histogram(hypo_bylang,50)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc_tst)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((750000,), (750000,), (750000,))\n",
      "((15000, 49), (15000, 49), (15000,), (735000, 49), (735000, 49), (735000,))\n"
     ]
    }
   ],
   "source": [
    "# making pair of train i-vector with mean of each language i-vector\n",
    "#  example : for total 3 ivectors\n",
    "#  ivector   ivector_p  label\n",
    "#     1         1         1\n",
    "#     1         2         0\n",
    "#     1         3         0\n",
    "#     2         1         0\n",
    "#     2         2         1\n",
    "#     ...      ...       ...\n",
    "#     3         3         1\n",
    "\n",
    "# preparing pair labels\n",
    "sim = []\n",
    "pair_a_idx = []\n",
    "pair_b_idx = []\n",
    "\n",
    "for i, lang in enumerate(languages):\n",
    "    for j, label in enumerate(train_languages_num):\n",
    "        pair_a_idx.append(i)\n",
    "        pair_b_idx.append(j)\n",
    "        if i == label:\n",
    "            sim.append(1)\n",
    "        else:\n",
    "            sim.append(0)\n",
    "            \n",
    "print(np.shape(pair_a_idx),np.shape(pair_b_idx), np.shape(sim))\n",
    "pair_a_idx=np.array(pair_a_idx)\n",
    "pair_b_idx=np.array(pair_b_idx)\n",
    "sim = np.array(sim)\n",
    "\n",
    "#shuffling\n",
    "shuffleidx = np.arange(0,np.size(pair_a_idx))\n",
    "np.random.shuffle(shuffleidx)\n",
    "pair_a_idx = pair_a_idx[shuffleidx]\n",
    "pair_b_idx = pair_b_idx[shuffleidx]\n",
    "sim = sim[shuffleidx]\n",
    "\n",
    "\n",
    "data = []\n",
    "data_p = []\n",
    "    \n",
    "for iter in np.arange(0,np.size(sim)) :\n",
    "        data.append( avg_train_ivec[pair_a_idx[iter]] )\n",
    "        data_p.append( train_ivec[pair_b_idx[iter]] )\n",
    "\n",
    "data = np.array(data)\n",
    "data_p = np.array(data_p)\n",
    "\n",
    "# TRN dataset loading for feeding \n",
    "tar_data = data[sim==1]\n",
    "tar_data_p = data_p[sim==1]\n",
    "tar_sim = sim[sim==1]\n",
    "non_data = data[sim==0]\n",
    "non_data_p = data_p[sim==0]\n",
    "non_sim = sim[sim==0]\n",
    "print(tar_data.shape, tar_data_p.shape,tar_sim.shape,non_data.shape,non_data_p.shape,non_sim.shape)\n",
    "\n",
    "trn_tar = ivector_dataset.DataSet(tar_data,tar_sim)\n",
    "trn_tar_p = ivector_dataset.DataSet(tar_data_p,tar_sim)\n",
    "\n",
    "trn_non = ivector_dataset.DataSet(non_data,non_sim)\n",
    "trn_non_p = ivector_dataset.DataSet(non_data_p,non_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2940)\n",
      "(?, 1500)\n",
      "(?, 600)\n",
      "(?, 2940)\n",
      "(?, 1500)\n",
      "(?, 600)\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "sess = tf.InteractiveSession()\n",
    "siamese = siamese_model.siamese();\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step,\n",
    "                                           5000, 0.99, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(siamese.loss, global_step=global_step)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sls/u/swshon/tools/pytf/local/lib/python2.7/site-packages/numpy/core/numeric.py:531: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40: loss 0.988, Acc.: (DEV)0.466 (TST)0.420, lr : 0.01000\n",
      "Step 50: loss 0.983, Acc.: (DEV)0.519 (TST)0.467, lr : 0.01000\n",
      "Step 60: loss 0.983, Acc.: (DEV)0.568 (TST)0.515, lr : 0.01000\n",
      "Step 70: loss 0.980, Acc.: (DEV)0.597 (TST)0.544, lr : 0.01000\n",
      "Step 80: loss 0.983, Acc.: (DEV)0.607 (TST)0.554, lr : 0.01000\n",
      "Step 90: loss 0.974, Acc.: (DEV)0.623 (TST)0.564, lr : 0.01000\n",
      "Step 100: loss 0.985, Acc.: (DEV)0.635 (TST)0.572, lr : 0.01000\n",
      "Step 110: loss 0.974, Acc.: (DEV)0.639 (TST)0.583, lr : 0.01000\n",
      "Step 120: loss 0.975, Acc.: (DEV)0.648 (TST)0.589, lr : 0.01000\n",
      "Step 130: loss 0.976, Acc.: (DEV)0.650 (TST)0.592, lr : 0.01000\n",
      "Step 140: loss 0.966, Acc.: (DEV)0.658 (TST)0.600, lr : 0.01000\n",
      "Step 150: loss 0.971, Acc.: (DEV)0.662 (TST)0.604, lr : 0.01000\n",
      "Step 160: loss 0.974, Acc.: (DEV)0.666 (TST)0.606, lr : 0.01000\n",
      "Step 170: loss 0.968, Acc.: (DEV)0.670 (TST)0.611, lr : 0.01000\n",
      "Step 180: loss 0.969, Acc.: (DEV)0.673 (TST)0.615, lr : 0.01000\n",
      "Step 190: loss 0.964, Acc.: (DEV)0.680 (TST)0.619, lr : 0.01000\n",
      "Step 200: loss 0.968, Acc.: (DEV)0.696 (TST)0.637, lr : 0.01000\n",
      "Step 210: loss 0.958, Acc.: (DEV)0.703 (TST)0.644, lr : 0.01000\n",
      "Step 220: loss 0.960, Acc.: (DEV)0.712 (TST)0.649, lr : 0.01000\n",
      "Step 250: loss 0.953, Acc.: (DEV)0.718 (TST)0.655, lr : 0.01000\n",
      "Step 330: loss 0.924, Acc.: (DEV)0.734 (TST)0.673, lr : 0.01000\n",
      "Step 360: loss 0.907, Acc.: (DEV)0.733 (TST)0.674, lr : 0.01000\n",
      "Step 460: loss 0.825, Acc.: (DEV)0.750 (TST)0.680, lr : 0.01000\n",
      "Step 500: loss 0.790, Acc.: (DEV)0.753 (TST)0.681, lr : 0.01000\n",
      "Step 520: loss 0.720, Acc.: (DEV)0.754 (TST)0.685, lr : 0.01000\n",
      "Step 530: loss 0.731, Acc.: (DEV)0.760 (TST)0.689, lr : 0.01000\n",
      "Step 550: loss 0.676, Acc.: (DEV)0.768 (TST)0.696, lr : 0.01000\n",
      "Step 580: loss 0.647, Acc.: (DEV)0.762 (TST)0.697, lr : 0.01000\n",
      "Step 620: loss 0.597, Acc.: (DEV)0.776 (TST)0.711, lr : 0.01000\n",
      "Step 680: loss 0.558, Acc.: (DEV)0.790 (TST)0.720, lr : 0.01000\n",
      "Step 790: loss 0.581, Acc.: (DEV)0.802 (TST)0.739, lr : 0.01000\n",
      "Step 830: loss 0.627, Acc.: (DEV)0.806 (TST)0.741, lr : 0.01000\n",
      "Step 890: loss 0.659, Acc.: (DEV)0.821 (TST)0.754, lr : 0.01000\n",
      "Step 960: loss 0.567, Acc.: (DEV)0.816 (TST)0.758, lr : 0.01000\n",
      "Step 970: loss 0.601, Acc.: (DEV)0.831 (TST)0.768, lr : 0.01000\n",
      "Step 1060: loss 0.493, Acc.: (DEV)0.835 (TST)0.773, lr : 0.01000\n",
      "Step 1090: loss 0.540, Acc.: (DEV)0.845 (TST)0.777, lr : 0.01000\n",
      "Step 1100: loss 0.654, Acc.: (DEV)0.847 (TST)0.777, lr : 0.01000\n",
      "Step 1110: loss 0.587, Acc.: (DEV)0.839 (TST)0.778, lr : 0.01000\n",
      "Step 1180: loss 0.521, Acc.: (DEV)0.848 (TST)0.782, lr : 0.01000\n",
      "Step 1250: loss 0.604, Acc.: (DEV)0.850 (TST)0.782, lr : 0.01000\n",
      "Step 1260: loss 0.457, Acc.: (DEV)0.852 (TST)0.785, lr : 0.01000\n",
      "Step 1300: loss 0.517, Acc.: (DEV)0.849 (TST)0.787, lr : 0.01000\n",
      "Step 1350: loss 0.490, Acc.: (DEV)0.851 (TST)0.787, lr : 0.01000\n",
      "Step 1380: loss 0.586, Acc.: (DEV)0.856 (TST)0.789, lr : 0.01000\n",
      "Step 1460: loss 0.610, Acc.: (DEV)0.852 (TST)0.789, lr : 0.01000\n",
      "Step 1530: loss 0.418, Acc.: (DEV)0.855 (TST)0.792, lr : 0.01000\n",
      "Step 1600: loss 0.537, Acc.: (DEV)0.870 (TST)0.802, lr : 0.01000\n",
      "Step 1860: loss 0.555, Acc.: (DEV)0.872 (TST)0.804, lr : 0.01000\n",
      "Step 1940: loss 0.502, Acc.: (DEV)0.875 (TST)0.808, lr : 0.01000\n",
      "Step 2570: loss 0.458, Acc.: (DEV)0.877 (TST)0.809, lr : 0.01000\n",
      "Step 2770: loss 0.482, Acc.: (DEV)0.878 (TST)0.809, lr : 0.01000\n",
      "Step 2920: loss 0.446, Acc.: (DEV)0.876 (TST)0.810, lr : 0.01000\n",
      "Step 3260: loss 0.441, Acc.: (DEV)0.881 (TST)0.813, lr : 0.01000\n",
      "Step 3350: loss 0.470, Acc.: (DEV)0.884 (TST)0.816, lr : 0.01000\n",
      "Step 5110: loss 0.389, Acc.: (DEV)0.894 (TST)0.819, lr : 0.00990\n",
      "Step 5850: loss 0.331, Acc.: (DEV)0.896 (TST)0.819, lr : 0.00990\n",
      "Step 6200: loss 0.511, Acc.: (DEV)0.897 (TST)0.823, lr : 0.00990\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "batch_size = 32\n",
    "max_acc = 0.40\n",
    "max_step=0\n",
    "saver_folder='snnmodel_ivector'\n",
    "if not os.path.exists(saver_folder):\n",
    "    os.mkdir(saver_folder)\n",
    "for step in range(100000):\n",
    "  \n",
    "    if step %1 ==0:\n",
    "        batch_x1_a, batch_y1_a = trn_tar.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_a, batch_y2_a = trn_tar_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1_b, batch_y1_b = trn_non.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_b, batch_y2_b = trn_non_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1 = np.append(batch_x1_a,batch_x1_b,axis=0)\n",
    "        batch_y1 = np.append(batch_y1_a,batch_y1_b,axis=0)\n",
    "        batch_x2 = np.append(batch_x2_a,batch_x2_b,axis=0)\n",
    "        batch_y2 = np.append(batch_y2_a,batch_y2_b,axis=0)\n",
    "    else:\n",
    "        batch_x1_a, batch_y1_a = dev_tar.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_a, batch_y2_a = dev_tar_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1_b, batch_y1_b = dev_non.next_batch(batch_size,shuffle=False)\n",
    "        batch_x2_b, batch_y2_b = dev_non_p.next_batch(batch_size,shuffle=False)\n",
    "        batch_x1 = np.append(batch_x1_a,batch_x1_b,axis=0)\n",
    "        batch_y1 = np.append(batch_y1_a,batch_y1_b,axis=0)\n",
    "        batch_x2 = np.append(batch_x2_a,batch_x2_b,axis=0)\n",
    "        batch_y2 = np.append(batch_y2_a,batch_y2_b,axis=0)\n",
    "        \n",
    "\n",
    "#     batch_x1,batch_y1 = mgb3_siam1.train.next_batch(120,shuffle=False)\n",
    "#     batch_x2,batch_y2 = mgb3_siam2.train.next_batch(120,shuffle=False)    \n",
    "#     batch_y = (batch_y1==batch_y2).astype('float')\n",
    "    batch_y = batch_y1*2-1\n",
    "#     batch_y = 1-batch_y1\n",
    "    \n",
    "    _, loss_v = sess.run([train_step, siamese.loss], feed_dict={\n",
    "        siamese.x1: batch_x1,\n",
    "        siamese.x2: batch_x2,\n",
    "        siamese.y_: batch_y\n",
    "    })\n",
    "    \n",
    "    if np.isnan(loss_v):\n",
    "        print ('Model diverged with loss = NAN')\n",
    "        quit()\n",
    "        \n",
    "    if step % 10 ==0:\n",
    "        trn_ivectors_siam = siamese.o1.eval({siamese.x1:train_ivec})\n",
    "        lang_mean_siam = siamese.o1.eval({siamese.x1:avg_train_ivec})\n",
    "        tst_ivectors_siam = siamese.o1.eval({siamese.x1:test_ivec})\n",
    "        trn_scores = lang_mean_siam.dot(trn_ivectors_siam.transpose() )\n",
    "        hypo_lang = np.argmax(trn_scores,axis = 0)\n",
    "        temp = ((train_languages_num) - hypo_lang)\n",
    "        acc =1- np.size(np.nonzero(temp)) / float( np.size(train_languages_num) )\n",
    "        \n",
    "        tst_scores = lang_mean_siam.dot(tst_ivectors_siam.transpose() )\n",
    "        hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "        hypo_lang = hypo_lang[(test_languages=='out_of_set')==False]\n",
    "\n",
    "        temp = ((test_languages_num) - hypo_lang)\n",
    "        acc_tst =1- np.size(np.nonzero(temp)) / float(np.size(test_languages_num))\n",
    "\n",
    "        if max_acc < acc_tst:\n",
    "            max_acc = acc_tst\n",
    "            max_step=step\n",
    "            print ('Step %d: loss %.3f, Acc.: (DEV)%.3f (TST)%.3f, lr : %.5f' % (step,loss_v,acc,acc_tst,sess.run(learning_rate)))\n",
    "            saver.save(sess, saver_folder+'/model'+str(step)+'.ckpt')\n",
    "        if loss_v<0.3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200\n",
      "INFO:tensorflow:Restoring parameters from snnmodel_ivector/model6200.ckpt\n",
      "Final accurary on test dataset : 0.823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saver_folder='snnmodel_ivector'\n",
    "print max_step\n",
    "RESTORE_STEP=max_step\n",
    "saver.restore(sess, saver_folder+'/model'+str(RESTORE_STEP)+'.ckpt')\n",
    "\n",
    "\n",
    "trn_ivectors_siam = siamese.o1.eval({siamese.x1:train_ivec})\n",
    "dev_ivectors_siam = siamese.o1.eval({siamese.x1:dev_ivec})\n",
    "tst_ivectors_siam = siamese.o1.eval({siamese.x1:test_ivec})\n",
    "lang_mean_siam = siamese.o1.eval({siamese.x1:avg_train_ivec})\n",
    "\n",
    "tst_scores = lang_mean_siam.dot(tst_ivectors_siam.transpose() )\n",
    "# print(tst_scores.shape)\n",
    "hypo_lang = np.argmax(tst_scores,axis = 0)\n",
    "hypo_lang = hypo_lang[(test_languages=='out_of_set')==False]\n",
    "temp = ((test_languages_num) - hypo_lang)\n",
    "acc =1- np.size(np.nonzero(temp)) / float(np.size(test_languages_num))\n",
    "print 'Final accurary on test dataset : %0.3f' %(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 77.   1.   1. ...,   5.   3.   1.]\n",
      " [  0.  81.   0. ...,   0.   1.   0.]\n",
      " [  4.   0.  97. ...,   2.   0.   0.]\n",
      " ..., \n",
      " [  0.   0.   0. ...,   0.   3.   0.]\n",
      " [  0.   0.   0. ...,  59.   0.   0.]\n",
      " [  1.   3.   1. ...,   1.  70.  89.]]\n",
      "Precision\n",
      "[ 0.23692308  0.87096774  0.49489796  0.76635514  0.07407407  0.25        0.\n",
      "  0.42574257  0.71311475  0.          0.          0.08333333  0.\n",
      "  0.06666667  0.          0.          0.68        0.          0.69642857\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.01020408  0.25490196  0.          0.86138614\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.79439252  0.          0.          0.          0.\n",
      "  0.46067416  0.          0.          0.          0.2124105 ]\n",
      "Recall\n",
      "[ 0.77  0.81  0.97  0.82  0.02  0.01  0.    0.86  0.87  0.    0.    0.02\n",
      "  0.    0.01  0.    0.    0.85  0.    0.78  0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.01  0.65  0.    0.87  0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.85  0.    0.    0.    0.    0.82  0.    0.    0.    0.89]\n",
      "\n",
      "\n",
      "<Performance evaluation on Test dataset>\n",
      "Accurary  : 0.823\n",
      "Precision : 0.159\n",
      "Recall    : 0.218\n"
     ]
    }
   ],
   "source": [
    "confusionmat = np.zeros((50,50))\n",
    "for i,lang in enumerate(languages):\n",
    "    hypo_bylang = hypo_lang[ test_languages_num == i]\n",
    "    hist_bylang = np.histogram(hypo_bylang,50)\n",
    "    confusionmat[:,i] = hist_bylang[0]\n",
    "\n",
    "precision = np.diag(confusionmat) / np.sum(confusionmat,axis=1) #precision\n",
    "recall = np.diag(confusionmat) / np.sum(confusionmat,axis=0) # recall\n",
    "    \n",
    "print 'Confusion matrix'\n",
    "print confusionmat\n",
    "print 'Precision'\n",
    "print precision\n",
    "print 'Recall'\n",
    "print recall\n",
    "\n",
    "print '\\n\\n<Performance evaluation on Test dataset>'\n",
    "print 'Accurary  : %0.3f' %(acc)\n",
    "print 'Precision : %0.3f' %(np.mean(precision))\n",
    "print 'Recall    : %0.3f' %(np.mean(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
